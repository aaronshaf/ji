import chalk from 'chalk';
import { ConfigManager } from '../../lib/config.js';

export async function configureModels() {
  console.log(chalk.bold('\nü§ñ Configure AI Models\n'));

  const configManager = new ConfigManager();

  try {
    // Get current settings
    const currentSettings = await configManager.getSettings();

    console.log(chalk.cyan('Current Settings:'));
    console.log(`  Ask Model: ${chalk.white(currentSettings.askModel || 'gemma2:latest (default)')}`);
    console.log(`  Embedding Model: ${chalk.white(currentSettings.embeddingModel || 'mxbai-embed-large (default)')}`);
    console.log(`  Analysis Model: ${chalk.white(currentSettings.analysisModel || 'gemma2:latest (default)')}`);

    console.log(chalk.dim('\nNote: You can modify these settings in the configuration file.'));

    console.log(chalk.yellow('\nüí° Available Models (install with ollama pull <model>):'));
    console.log(chalk.white('  Chat Models: gemma2:latest, llama3.1, qwen2.5, phi3.5'));
    console.log(chalk.white('  Embedding Models: mxbai-embed-large, nomic-embed-text, all-minilm'));

    console.log(chalk.yellow('\n‚ö†Ô∏è  Requirements:'));
    console.log(chalk.red('  ‚Ä¢ Ensure Ollama is running: ollama serve'));
    console.log(chalk.red('  ‚Ä¢ Install models first: ollama pull <model-name>'));
  } finally {
    configManager.close();
  }
}
